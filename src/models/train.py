import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import sys
import time

# Import c√°c module c·ªßa ch√∫ng ta
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))
import src.config as config
from src.models.model import TimeSeriesTransformer
from src.data.data_loader import get_data_loaders


def train_model():
    print("üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH HU·∫§N LUY·ªÜN MODEL...")

    # 1. Ch·ªçn thi·∫øt b·ªã (GPU n·∫øu c√≥)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"‚öôÔ∏è  Thi·∫øt b·ªã s·ª≠ d·ª•ng: {device}")

    # 2. Load D·ªØ li·ªáu
    train_loader, val_loader, _ = get_data_loaders()

    # L·∫•y m·∫´u ƒë·ªÉ xem s·ªë chi·ªÅu input
    X_sample, _ = next(iter(train_loader))
    num_features = X_sample.shape[2]  # S·∫Ω l√† 6
    print(f"‚ÑπÔ∏è  S·ªë ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (Features): {num_features}")

    # 3. Kh·ªüi t·∫°o Model
    model = TimeSeriesTransformer(
        num_features=num_features,
        d_model=config.D_MODEL,
        nhead=config.N_HEADS,
        num_encoder_layers=config.NUM_ENCODER_LAYERS,
        dim_feedforward=config.DIM_FEEDFORWARD,
        dropout=config.DROPOUT_RATE,
        input_window=config.INPUT_WINDOW_DAYS,
        output_window=config.OUTPUT_WINDOW_DAYS
    ).to(device)

    # 4. C√†i ƒë·∫∑t Loss & Optimizer
    criterion = nn.MSELoss()  # D√πng Mean Squared Error cho b√†i to√°n h·ªìi quy
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)

    # Learning Rate Scheduler: Gi·∫£m LR n·∫øu loss kh√¥ng gi·∫£m (gi√∫p h·ªôi t·ª• t·ªët h∆°n)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

    # 5. V√≤ng l·∫∑p Training
    best_val_loss = float('inf')

    for epoch in range(config.EPOCHS):
        start_time = time.time()

        # --- TRAINING ---
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            output = model(X_batch)  # Forward
            loss = criterion(output, y_batch)  # T√≠nh l·ªói
            loss.backward()  # Backward
            optimizer.step()  # C·∫≠p nh·∫≠t tr·ªçng s·ªë

            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)

        # --- VALIDATION ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                output = model(X_batch)
                loss = criterion(output, y_batch)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        # C·∫≠p nh·∫≠t LR
        scheduler.step(avg_val_loss)

        # --- LOGGING & SAVING ---
        epoch_time = time.time() - start_time
        print(
            f"Epoch {epoch + 1}/{config.EPOCHS} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Time: {epoch_time:.2f}s")

        # L∆∞u model t·ªët nh·∫•t (Checkpoint)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), config.MODEL_SAVE_PATH)
            print(f"   üî• ƒê√£ l∆∞u model t·ªët nh·∫•t (Val Loss gi·∫£m t·ª´ {best_val_loss:.6f} -> {avg_val_loss:.6f})")

    print("\n‚úÖ HU·∫§N LUY·ªÜN HO√ÄN T·∫§T!")
    print(f"Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {config.MODEL_SAVE_PATH}")


if __name__ == "__main__":
    train_model()